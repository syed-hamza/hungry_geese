{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dedb294-b121-4f4b-a08f-e033088eb0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment football failed: No module named 'gfootball'\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from kaggle_environments.envs.hungry_geese.hungry_geese import Observation,\\\n",
    "Configuration, Action, row_col, adjacent_positions, translate, min_distance,random_agent, GreedyAgent\n",
    "import os\n",
    "import numpy as np\n",
    "from kaggle_environments import make\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14044170-a969-49d8-8af6-df0dc70c45ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 reward:  1402\n",
      "episode:  1 reward:  501\n",
      "episode:  2 reward:  1301\n",
      "episode:  3 reward:  201\n",
      "episode:  4 reward:  1501\n",
      "episode:  5 reward:  301\n",
      "episode:  6 reward:  201\n",
      "episode:  7 reward:  801\n",
      "episode:  8 reward:  501\n",
      "episode:  9 reward:  301\n"
     ]
    }
   ],
   "source": [
    "env = make('hungry_geese')\n",
    "trainer = env.train([None, 'greedy', 'greedy', 'greedy'])\n",
    "episodes = 10\n",
    "for i in range(episodes):\n",
    "    state = env.reset(num_agents = 4)\n",
    "    done = False\n",
    "    score = 0\n",
    "#    env.render(mode=\"ipython\", width=500, height=450)\n",
    "    while not done:\n",
    "        legend = {0:'NORTH',1:'EAST',2:'SOUTH',3:'WEST'}\n",
    "        action = random.randint(0,3)\n",
    "        obs,reward, done, info = trainer.step(legend[action])\n",
    "        score +=reward\n",
    "    print('episode: ',i,'reward: ',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34abdb66-bd19-4155-90aa-8b9ebf5c4f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 7, 11, 21)         0         \n",
      "_________________________________________________________________\n",
      "input_2 (InputLayer)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 5, 7, 32)          10112     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 5, 7, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 5, 7, 32)          1056      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 5, 7, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 3, 3, 64)          30784     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 3, 3, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 3, 3, 64)          4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 3, 3, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 1, 1, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 1, 1, 128)         16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 36        \n",
      "=================================================================\n",
      "Total params: 148,908\n",
      "Trainable params: 148,012\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def DQNet(obs,action,shape):\n",
    "    model = tf.keras.Sequential()\n",
    "    if shape == 5:\n",
    "        model.add(tf.keras.layers.Reshape(obs, input_shape=(1,7,11,21)))\n",
    "        \n",
    "    model.add(tf.keras.Input(shape=obs))\n",
    "    model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,5),strides = 1,activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=1,strides = 1, activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,5),strides = 1, activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=1,strides = 1, activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3,strides = 1, activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=1,strides = 1, activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "#     model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(action, activation='linear'))\n",
    "    \n",
    "#    model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "model = DQNet([7,11,21],4,5)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f32e8c7c-c447-46fe-9b61-f1e2168e3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "place = 0\n",
    "def get_nearest_cells(x,y):\n",
    "    # returns all cells reachable from the current one\n",
    "        result = []\n",
    "        for i in (-1,+1):\n",
    "            result.append(((x+i+7)%7, y))\n",
    "            result.append((x,(y+i+11)%11))\n",
    "        return result\n",
    "def change_reward(done,agent_value,obs,prev_action,prev_food):\n",
    "    global place\n",
    "    try:\n",
    "        observation = obs[-1]['observation']\n",
    "    except:\n",
    "        observation = obs[-1]\n",
    "    player_index = observation['index']\n",
    "    rewards = [500,200,80,0]\n",
    "    if len(observation['geese'][player_index]) == 0:\n",
    "        alive = [i for i in observation['geese'] if len(i)>0]\n",
    "        place = len(alive)\n",
    "    if done:\n",
    "        reward = rewards[place]\n",
    "        place = 0\n",
    "        return reward\n",
    "    else:\n",
    "        player_pos =  observation['geese'][player_index][0]\n",
    "        if player_pos in prev_food:\n",
    "            return 5\n",
    "        else:\n",
    "            return 1\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb736db8-17f5-461e-aff1-2434c5059b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_action = None\n",
    "prev_food = []\n",
    "class HungryGeeseEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.env = make(\"hungry_geese\")\n",
    "        self.config = self.env.configuration\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Box(low=-1,\n",
    "                                            high=1, \n",
    "                                            shape=(self.config.rows, \n",
    "                                                   self.config.columns, \n",
    "                                                   1), \n",
    "                                            dtype=np.float64)\n",
    "        self.trainer = self.env.train([None,'greedy','greedy','greedy'])\n",
    "        self.NUM_AGENTS = 4\n",
    "    def transform_obs(self,obs_dict, config_dict):\n",
    "        try:\n",
    "            observation = obs_dict[-1]['observation']\n",
    "        except:\n",
    "            observation = obs_dict[-1]\n",
    "        food_pos = []\n",
    "        player = observation['index']\n",
    "        final_table = np.zeros((21, 7 * 11), dtype=np.float32)\n",
    "        for p, geese in enumerate(observation['geese']):\n",
    "            # head position\n",
    "            for pos in geese[:1]:\n",
    "                final_table[0 + (p - player) % self.NUM_AGENTS, pos] = 1\n",
    "            # tip position\n",
    "            for pos in geese[-1:]:\n",
    "                final_table[4 + (p - player) % self.NUM_AGENTS, pos] = 1\n",
    "            # whole position\n",
    "            for pos in geese:\n",
    "                final_table[8 + (p - player) % self.NUM_AGENTS, pos] = 1\n",
    "            try:    \n",
    "                x,y = row_col(geese[0], 11)\n",
    "                possible_moves = get_nearest_cells(x,y) \n",
    "\n",
    "                for i in possible_moves:\n",
    "                    pos = x*11 + y\n",
    "                    final_table[17 + (p - player) % self.NUM_AGENTS,pos] = 1 \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # previous head position\n",
    "        if len(geese)>0:\n",
    "                x,y = row_col(geese[0], 11)\n",
    "                possible_moves = get_nearest_cells(x,y) \n",
    "\n",
    "                for i in possible_moves:\n",
    "                    pos = x*11 + y\n",
    "                    final_table[17 + (p - player) % self.NUM_AGENTS,pos] = 1 \n",
    "\n",
    "        # food\n",
    "        for pos in observation['food']:\n",
    "            final_table[16, pos] = 1\n",
    "            food_pos.append(pos)\n",
    "        return np.transpose(final_table.reshape(-1,7,11),(1,2,0)),food_pos\n",
    "    \n",
    "    def reset(self):\n",
    "        self.obs = []\n",
    "        self.obs.append(self.env.reset(num_agents = 4)[0])\n",
    "        self.transformed_obs,food_data = self.transform_obs(self.obs,self.config)\n",
    "        return self.transformed_obs\n",
    "    \n",
    "    def transform_action(self,agent_value):\n",
    "        # We need to transform the numerical values retruned by the algorithm \n",
    "        #into string values accepted by the default env \n",
    "        if agent_value == 0:\n",
    "            return \"NORTH\"\n",
    "        if agent_value == 1:\n",
    "            return \"EAST\"\n",
    "        if agent_value == 2:\n",
    "            return \"WEST\"\n",
    "        if agent_value == 3:\n",
    "            return \"SOUTH\"\n",
    "    \n",
    "    def step(self, agent_value):\n",
    "        global prev_action\n",
    "        global prev_food\n",
    "        food_data = []\n",
    "        action = self.transform_action(agent_value)\n",
    "        current_obs, reward, done, info = self.trainer.step(action)\n",
    "        self.obs.append(current_obs)\n",
    "        if prev_action is None:\n",
    "            prev_action = agent_value\n",
    "        if done == False:\n",
    "            self.transformed_obs,food_data = self.transform_obs(self.obs, self.config)\n",
    "        reward = change_reward(done,agent_value,self.obs,prev_action,prev_food)\n",
    "        prev_food = food_data\n",
    "        prev_action = action\n",
    "        return self.transformed_obs, reward, done, info\n",
    "env = HungryGeeseEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d122a9c-7a24-424c-a7ba-7ca378e3470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit = 1000000, window_length = 1)\n",
    "    dqn = DQNAgent(model = model, memory = memory, policy = policy,nb_actions = 4, nb_steps_warmup = 100, target_model_update = 1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec76adc2-34d1-4781-8a27-002a3133858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsyed/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     6/50000: episode: 1, duration: 2.073s, episode steps:   6, steps per second:   3, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 1.167 [0.000, 2.000],  loss: --, mae: --, mean_q: --\n",
      "     9/50000: episode: 2, duration: 0.023s, episode steps:   3, steps per second: 129, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.000 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    17/50000: episode: 3, duration: 0.042s, episode steps:   8, steps per second: 192, episode reward:  0.700, mean reward:  0.088 [ 0.000,  0.100], mean action: 1.875 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    21/50000: episode: 4, duration: 0.025s, episode steps:   4, steps per second: 160, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.250 [0.000, 2.000],  loss: --, mae: --, mean_q: --\n",
      "    27/50000: episode: 5, duration: 0.033s, episode steps:   6, steps per second: 184, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 1.167 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    32/50000: episode: 6, duration: 0.028s, episode steps:   5, steps per second: 176, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 0.800 [0.000, 2.000],  loss: --, mae: --, mean_q: --\n",
      "    36/50000: episode: 7, duration: 0.025s, episode steps:   4, steps per second: 158, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.750 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    41/50000: episode: 8, duration: 0.029s, episode steps:   5, steps per second: 175, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 1.400 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    45/50000: episode: 9, duration: 0.025s, episode steps:   4, steps per second: 163, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.250 [0.000, 2.000],  loss: --, mae: --, mean_q: --\n",
      "    48/50000: episode: 10, duration: 0.021s, episode steps:   3, steps per second: 146, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.667 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    51/50000: episode: 11, duration: 0.020s, episode steps:   3, steps per second: 147, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.333 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    53/50000: episode: 12, duration: 0.016s, episode steps:   2, steps per second: 124, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    57/50000: episode: 13, duration: 0.024s, episode steps:   4, steps per second: 165, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.000 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    61/50000: episode: 14, duration: 0.024s, episode steps:   4, steps per second: 164, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.750 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    64/50000: episode: 15, duration: 0.020s, episode steps:   3, steps per second: 148, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: --, mae: --, mean_q: --\n",
      "    68/50000: episode: 16, duration: 0.025s, episode steps:   4, steps per second: 161, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.250 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    70/50000: episode: 17, duration: 0.016s, episode steps:   2, steps per second: 123, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "    75/50000: episode: 18, duration: 0.030s, episode steps:   5, steps per second: 166, episode reward:  0.500, mean reward:  0.100 [ 0.000,  0.200], mean action: 1.200 [0.000, 2.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsyed/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   103/50000: episode: 19, duration: 7.540s, episode steps:  28, steps per second:   4, episode reward:  2.700, mean reward:  0.096 [ 0.000,  0.100], mean action: 1.286 [0.000, 3.000],  loss: 0.077055, mae: 0.362684, mean_q: 0.635730\n",
      "   123/50000: episode: 20, duration: 0.493s, episode steps:  20, steps per second:  41, episode reward:  2.000, mean reward:  0.100 [ 0.000,  0.200], mean action: 1.150 [0.000, 3.000],  loss: 0.036437, mae: 0.276040, mean_q: 0.464634\n",
      "   131/50000: episode: 21, duration: 0.209s, episode steps:   8, steps per second:  38, episode reward:  0.800, mean reward:  0.100 [ 0.000,  0.200], mean action: 2.000 [0.000, 3.000],  loss: 0.012524, mae: 0.214168, mean_q: 0.357289\n",
      "   135/50000: episode: 22, duration: 0.117s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.750 [1.000, 3.000],  loss: 0.006998, mae: 0.179702, mean_q: 0.319817\n",
      "   145/50000: episode: 23, duration: 0.254s, episode steps:  10, steps per second:  39, episode reward:  0.900, mean reward:  0.090 [ 0.000,  0.100], mean action: 1.800 [0.000, 3.000],  loss: 0.009389, mae: 0.176492, mean_q: 0.313778\n",
      "   147/50000: episode: 24, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 0.007080, mae: 0.167117, mean_q: 0.300412\n",
      "   149/50000: episode: 25, duration: 0.075s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 0.008610, mae: 0.185222, mean_q: 0.337389\n",
      "   152/50000: episode: 26, duration: 0.094s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 0.006387, mae: 0.192490, mean_q: 0.331543\n",
      "   158/50000: episode: 27, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 0.007113, mae: 0.161462, mean_q: 0.277771\n",
      "   164/50000: episode: 28, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 0.833 [0.000, 3.000],  loss: 0.007498, mae: 0.153026, mean_q: 0.266610\n",
      "   167/50000: episode: 29, duration: 0.100s, episode steps:   3, steps per second:  30, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.333 [1.000, 2.000],  loss: 0.005922, mae: 0.139465, mean_q: 0.230865\n",
      "   171/50000: episode: 30, duration: 0.118s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.250 [0.000, 3.000],  loss: 0.011642, mae: 0.161173, mean_q: 0.263715\n",
      "   176/50000: episode: 31, duration: 0.140s, episode steps:   5, steps per second:  36, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 0.800 [0.000, 3.000],  loss: 0.005923, mae: 0.145034, mean_q: 0.239978\n",
      "   183/50000: episode: 32, duration: 0.185s, episode steps:   7, steps per second:  38, episode reward:  0.700, mean reward:  0.100 [ 0.000,  0.200], mean action: 1.714 [1.000, 3.000],  loss: 0.004953, mae: 0.151904, mean_q: 0.257619\n",
      "   186/50000: episode: 33, duration: 0.098s, episode steps:   3, steps per second:  30, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 0.005993, mae: 0.134947, mean_q: 0.245597\n",
      "   191/50000: episode: 34, duration: 0.140s, episode steps:   5, steps per second:  36, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 1.600 [0.000, 3.000],  loss: 0.003809, mae: 0.126520, mean_q: 0.217976\n",
      "   194/50000: episode: 35, duration: 0.094s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.667 [0.000, 3.000],  loss: 0.002788, mae: 0.132488, mean_q: 0.230769\n",
      "   197/50000: episode: 36, duration: 0.096s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 0.002614, mae: 0.132183, mean_q: 0.231258\n",
      "   200/50000: episode: 37, duration: 0.098s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.333 [0.000, 3.000],  loss: 0.003338, mae: 0.130783, mean_q: 0.241474\n",
      "   216/50000: episode: 38, duration: 0.409s, episode steps:  16, steps per second:  39, episode reward:  1.500, mean reward:  0.094 [ 0.000,  0.100], mean action: 2.188 [1.000, 3.000],  loss: 0.003515, mae: 0.134507, mean_q: 0.232059\n",
      "   221/50000: episode: 39, duration: 0.139s, episode steps:   5, steps per second:  36, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 1.800 [0.000, 3.000],  loss: 0.003391, mae: 0.129343, mean_q: 0.234585\n",
      "   227/50000: episode: 40, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 0.833 [0.000, 3.000],  loss: 0.004087, mae: 0.134417, mean_q: 0.239003\n",
      "   243/50000: episode: 41, duration: 0.395s, episode steps:  16, steps per second:  41, episode reward:  1.500, mean reward:  0.094 [ 0.000,  0.100], mean action: 1.625 [0.000, 3.000],  loss: 0.003575, mae: 0.137568, mean_q: 0.244665\n",
      "   245/50000: episode: 42, duration: 0.073s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 0.003565, mae: 0.135869, mean_q: 0.226707\n",
      "   249/50000: episode: 43, duration: 0.117s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 2.000 [0.000, 3.000],  loss: 0.003717, mae: 0.133031, mean_q: 0.245328\n",
      "   252/50000: episode: 44, duration: 0.096s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.333 [0.000, 3.000],  loss: 0.003508, mae: 0.142699, mean_q: 0.244087\n",
      "   258/50000: episode: 45, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 1.500 [0.000, 2.000],  loss: 0.003464, mae: 0.140593, mean_q: 0.251720\n",
      "   264/50000: episode: 46, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 2.167 [1.000, 3.000],  loss: 0.003133, mae: 0.138914, mean_q: 0.249115\n",
      "   266/50000: episode: 47, duration: 0.073s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 0.002631, mae: 0.139267, mean_q: 0.253024\n",
      "   277/50000: episode: 48, duration: 0.279s, episode steps:  11, steps per second:  39, episode reward:  1.000, mean reward:  0.091 [ 0.000,  0.100], mean action: 1.455 [0.000, 3.000],  loss: 0.003855, mae: 0.144878, mean_q: 0.268090\n",
      "   281/50000: episode: 49, duration: 0.117s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.250 [1.000, 2.000],  loss: 0.002944, mae: 0.140493, mean_q: 0.271281\n",
      "   288/50000: episode: 50, duration: 0.196s, episode steps:   7, steps per second:  36, episode reward:  0.600, mean reward:  0.086 [ 0.000,  0.100], mean action: 0.571 [0.000, 3.000],  loss: 0.004991, mae: 0.148352, mean_q: 0.273733\n",
      "   297/50000: episode: 51, duration: 0.231s, episode steps:   9, steps per second:  39, episode reward:  0.800, mean reward:  0.089 [ 0.000,  0.100], mean action: 2.111 [1.000, 3.000],  loss: 0.003430, mae: 0.142483, mean_q: 0.265524\n",
      "   301/50000: episode: 52, duration: 0.116s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 2.000 [0.000, 3.000],  loss: 0.003683, mae: 0.143223, mean_q: 0.265375\n",
      "   303/50000: episode: 53, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 0.003420, mae: 0.147442, mean_q: 0.285056\n",
      "   308/50000: episode: 54, duration: 0.147s, episode steps:   5, steps per second:  34, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 0.003534, mae: 0.143304, mean_q: 0.273930\n",
      "   310/50000: episode: 55, duration: 0.073s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 0.003062, mae: 0.142138, mean_q: 0.264185\n",
      "   312/50000: episode: 56, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 0.002733, mae: 0.153207, mean_q: 0.279242\n",
      "   320/50000: episode: 57, duration: 0.210s, episode steps:   8, steps per second:  38, episode reward:  0.700, mean reward:  0.088 [ 0.000,  0.100], mean action: 1.250 [0.000, 3.000],  loss: 0.003376, mae: 0.151815, mean_q: 0.291517\n",
      "   322/50000: episode: 58, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 0.002695, mae: 0.150902, mean_q: 0.276243\n",
      "   324/50000: episode: 59, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 0.002898, mae: 0.140490, mean_q: 0.270277\n",
      "   333/50000: episode: 60, duration: 0.231s, episode steps:   9, steps per second:  39, episode reward:  0.800, mean reward:  0.089 [ 0.000,  0.100], mean action: 1.444 [0.000, 3.000],  loss: 0.003945, mae: 0.152627, mean_q: 0.291093\n",
      "   337/50000: episode: 61, duration: 0.117s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 0.003382, mae: 0.147209, mean_q: 0.288893\n",
      "   340/50000: episode: 62, duration: 0.112s, episode steps:   3, steps per second:  27, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 0.003828, mae: 0.167034, mean_q: 0.301158\n",
      "   342/50000: episode: 63, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 0.002791, mae: 0.161883, mean_q: 0.297253\n",
      "   350/50000: episode: 64, duration: 0.209s, episode steps:   8, steps per second:  38, episode reward:  0.800, mean reward:  0.100 [ 0.000,  0.200], mean action: 1.750 [1.000, 3.000],  loss: 0.003297, mae: 0.158089, mean_q: 0.305543\n",
      "   352/50000: episode: 65, duration: 0.073s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 0.003485, mae: 0.157706, mean_q: 0.303004\n",
      "   355/50000: episode: 66, duration: 0.096s, episode steps:   3, steps per second:  31, episode reward: 33.200, mean reward: 11.067 [ 0.100, 33.000], mean action: 1.333 [0.000, 2.000],  loss: 0.004137, mae: 0.164678, mean_q: 0.304549\n",
      "   357/50000: episode: 67, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 0.003002, mae: 0.145269, mean_q: 0.298562\n",
      "   360/50000: episode: 68, duration: 0.094s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.333 [0.000, 3.000],  loss: 5.624332, mae: 0.241242, mean_q: 0.299971\n",
      "   379/50000: episode: 69, duration: 0.480s, episode steps:  19, steps per second:  40, episode reward:  1.800, mean reward:  0.095 [ 0.000,  0.100], mean action: 2.368 [1.000, 3.000],  loss: 0.006021, mae: 0.183010, mean_q: 0.333648\n",
      "   385/50000: episode: 70, duration: 0.173s, episode steps:   6, steps per second:  35, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 0.833 [0.000, 3.000],  loss: 2.830474, mae: 0.227118, mean_q: 0.348532\n",
      "   391/50000: episode: 71, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 1.333 [0.000, 3.000],  loss: 0.014171, mae: 0.225195, mean_q: 0.410779\n",
      "   397/50000: episode: 72, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 0.500 [0.000, 2.000],  loss: 2.417130, mae: 0.256124, mean_q: 0.417095\n",
      "   399/50000: episode: 73, duration: 0.071s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 5.931982, mae: 0.341737, mean_q: 0.526783\n",
      "   406/50000: episode: 74, duration: 0.188s, episode steps:   7, steps per second:  37, episode reward:  0.600, mean reward:  0.086 [ 0.000,  0.100], mean action: 1.143 [0.000, 3.000],  loss: 0.073812, mae: 0.317117, mean_q: 0.684188\n",
      "   410/50000: episode: 75, duration: 0.119s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.250 [0.000, 2.000],  loss: 2.037054, mae: 0.333643, mean_q: 0.676538\n",
      "   418/50000: episode: 76, duration: 0.214s, episode steps:   8, steps per second:  37, episode reward:  0.700, mean reward:  0.088 [ 0.000,  0.100], mean action: 1.125 [0.000, 3.000],  loss: 0.046546, mae: 0.349477, mean_q: 0.890893\n",
      "   424/50000: episode: 77, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 2.000 [0.000, 3.000],  loss: 0.263028, mae: 0.379539, mean_q: 0.939070\n",
      "   427/50000: episode: 78, duration: 0.097s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.333 [0.000, 3.000],  loss: 0.018563, mae: 0.349349, mean_q: 0.823729\n",
      "   431/50000: episode: 79, duration: 0.117s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 2.000 [0.000, 3.000],  loss: 0.065850, mae: 0.387455, mean_q: 0.933019\n",
      "   434/50000: episode: 80, duration: 0.095s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.333 [0.000, 3.000],  loss: 0.083669, mae: 0.469254, mean_q: 1.140802\n",
      "   436/50000: episode: 81, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 0.077491, mae: 0.508933, mean_q: 1.357580\n",
      "   438/50000: episode: 82, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 0.017495, mae: 0.477319, mean_q: 1.295863\n",
      "   440/50000: episode: 83, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 0.031889, mae: 0.520338, mean_q: 1.441812\n",
      "   454/50000: episode: 84, duration: 0.362s, episode steps:  14, steps per second:  39, episode reward:  1.300, mean reward:  0.093 [ 0.000,  0.100], mean action: 1.786 [0.000, 3.000],  loss: 0.388127, mae: 0.512199, mean_q: 1.351592\n",
      "   456/50000: episode: 85, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 0.779138, mae: 0.535529, mean_q: 1.549447\n",
      "   462/50000: episode: 86, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 2.167 [1.000, 3.000],  loss: 0.049624, mae: 0.458020, mean_q: 1.190017\n",
      "   468/50000: episode: 87, duration: 0.170s, episode steps:   6, steps per second:  35, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 1.500 [1.000, 3.000],  loss: 0.973435, mae: 0.458048, mean_q: 1.195236\n",
      "   470/50000: episode: 88, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 0.017678, mae: 0.407432, mean_q: 1.000679\n",
      "   475/50000: episode: 89, duration: 0.140s, episode steps:   5, steps per second:  36, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 0.800 [0.000, 2.000],  loss: 0.030939, mae: 0.460811, mean_q: 1.166596\n",
      "   483/50000: episode: 90, duration: 0.208s, episode steps:   8, steps per second:  38, episode reward:  0.700, mean reward:  0.088 [ 0.000,  0.100], mean action: 2.125 [1.000, 3.000],  loss: 0.779982, mae: 0.457702, mean_q: 1.184178\n",
      "   485/50000: episode: 91, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 0.023760, mae: 0.495122, mean_q: 1.301471\n",
      "   487/50000: episode: 92, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 0.053135, mae: 0.472562, mean_q: 1.274917\n",
      "   490/50000: episode: 93, duration: 0.095s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.000 [0.000, 3.000],  loss: 0.161688, mae: 0.520125, mean_q: 1.353723\n",
      "   492/50000: episode: 94, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 0.170576, mae: 0.468223, mean_q: 1.246888\n",
      "   494/50000: episode: 95, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 0.022335, mae: 0.552081, mean_q: 1.514795\n",
      "   496/50000: episode: 96, duration: 0.075s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 0.050477, mae: 0.496565, mean_q: 1.290123\n",
      "   501/50000: episode: 97, duration: 0.142s, episode steps:   5, steps per second:  35, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 1.000 [0.000, 3.000],  loss: 0.031897, mae: 0.524593, mean_q: 1.412883\n",
      "   503/50000: episode: 98, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 0.466568, mae: 0.561724, mean_q: 1.577025\n",
      "   506/50000: episode: 99, duration: 0.103s, episode steps:   3, steps per second:  29, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.667 [1.000, 2.000],  loss: 0.113617, mae: 0.480899, mean_q: 1.266885\n",
      "   509/50000: episode: 100, duration: 0.095s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 2.000 [0.000, 3.000],  loss: 0.690108, mae: 0.506696, mean_q: 1.224962\n",
      "   511/50000: episode: 101, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 0.022831, mae: 0.465933, mean_q: 1.176329\n",
      "   513/50000: episode: 102, duration: 0.079s, episode steps:   2, steps per second:  25, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 0.028761, mae: 0.453920, mean_q: 1.115483\n",
      "   522/50000: episode: 103, duration: 0.231s, episode steps:   9, steps per second:  39, episode reward: 33.900, mean reward:  3.767 [ 0.100, 33.000], mean action: 0.556 [0.000, 2.000],  loss: 0.260366, mae: 0.463642, mean_q: 1.048123\n",
      "   533/50000: episode: 104, duration: 0.288s, episode steps:  11, steps per second:  38, episode reward:  1.000, mean reward:  0.091 [ 0.000,  0.100], mean action: 2.182 [0.000, 3.000],  loss: 2.958838, mae: 0.518629, mean_q: 1.052074\n",
      "   535/50000: episode: 105, duration: 0.082s, episode steps:   2, steps per second:  24, episode reward: 33.100, mean reward: 16.550 [ 0.100, 33.000], mean action: 1.000 [1.000, 1.000],  loss: 0.343234, mae: 0.573905, mean_q: 1.351687\n",
      "   542/50000: episode: 106, duration: 0.189s, episode steps:   7, steps per second:  37, episode reward:  0.700, mean reward:  0.100 [ 0.000,  0.200], mean action: 0.714 [0.000, 3.000],  loss: 2.989386, mae: 0.652285, mean_q: 1.551473\n",
      "   544/50000: episode: 107, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 0.352283, mae: 0.680541, mean_q: 1.740631\n",
      "   549/50000: episode: 108, duration: 0.144s, episode steps:   5, steps per second:  35, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 1.000 [0.000, 3.000],  loss: 0.541362, mae: 0.695477, mean_q: 1.601482\n",
      "   559/50000: episode: 109, duration: 0.254s, episode steps:  10, steps per second:  39, episode reward:  0.900, mean reward:  0.090 [ 0.000,  0.100], mean action: 1.600 [0.000, 3.000],  loss: 0.222606, mae: 0.700390, mean_q: 1.717576\n",
      "   563/50000: episode: 110, duration: 0.118s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.250 [0.000, 3.000],  loss: 3.074704, mae: 0.801100, mean_q: 1.873602\n",
      "   566/50000: episode: 111, duration: 0.096s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.333 [0.000, 3.000],  loss: 0.061786, mae: 0.752285, mean_q: 1.902012\n",
      "   570/50000: episode: 112, duration: 0.119s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 2.000 [0.000, 3.000],  loss: 0.342692, mae: 0.585017, mean_q: 1.670839\n",
      "   575/50000: episode: 113, duration: 0.140s, episode steps:   5, steps per second:  36, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 2.000 [0.000, 3.000],  loss: 0.372541, mae: 0.716620, mean_q: 1.962175\n",
      "   580/50000: episode: 114, duration: 0.148s, episode steps:   5, steps per second:  34, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 1.800 [0.000, 3.000],  loss: 0.254150, mae: 0.871237, mean_q: 2.200709\n",
      "   583/50000: episode: 115, duration: 0.097s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.333 [1.000, 2.000],  loss: 4.015110, mae: 0.855328, mean_q: 1.915310\n",
      "   591/50000: episode: 116, duration: 0.209s, episode steps:   8, steps per second:  38, episode reward:  0.700, mean reward:  0.088 [ 0.000,  0.100], mean action: 1.875 [0.000, 3.000],  loss: 2.168673, mae: 0.855554, mean_q: 2.013924\n",
      "   596/50000: episode: 117, duration: 0.140s, episode steps:   5, steps per second:  36, episode reward: 33.400, mean reward:  6.680 [ 0.100, 33.000], mean action: 1.400 [0.000, 2.000],  loss: 5.595682, mae: 0.821225, mean_q: 1.708598\n",
      "   601/50000: episode: 118, duration: 0.143s, episode steps:   5, steps per second:  35, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 2.200 [0.000, 3.000],  loss: 18.886578, mae: 1.172855, mean_q: 2.030799\n",
      "   606/50000: episode: 119, duration: 0.140s, episode steps:   5, steps per second:  36, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 5.852934, mae: 1.055926, mean_q: 2.256473\n",
      "   608/50000: episode: 120, duration: 0.077s, episode steps:   2, steps per second:  26, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 0.493073, mae: 1.249598, mean_q: 2.522800\n",
      "   613/50000: episode: 121, duration: 0.140s, episode steps:   5, steps per second:  36, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 1.000 [0.000, 3.000],  loss: 0.781043, mae: 1.041567, mean_q: 2.348872\n",
      "   618/50000: episode: 122, duration: 0.146s, episode steps:   5, steps per second:  34, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 0.800 [0.000, 2.000],  loss: 0.375909, mae: 0.981106, mean_q: 2.256912\n",
      "   635/50000: episode: 123, duration: 0.414s, episode steps:  17, steps per second:  41, episode reward:  1.600, mean reward:  0.094 [ 0.000,  0.100], mean action: 1.235 [0.000, 3.000],  loss: 3.277196, mae: 0.903632, mean_q: 1.950368\n",
      "   638/50000: episode: 124, duration: 0.094s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 28.217302, mae: 1.051660, mean_q: 2.039995\n",
      "   656/50000: episode: 125, duration: 0.439s, episode steps:  18, steps per second:  41, episode reward: 67.700, mean reward:  3.761 [ 0.100, 66.000], mean action: 2.167 [0.000, 3.000],  loss: 5.404055, mae: 1.212365, mean_q: 2.459733\n",
      "   658/50000: episode: 126, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 2.253810, mae: 1.189278, mean_q: 3.475688\n",
      "   660/50000: episode: 127, duration: 0.074s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 1.017230, mae: 1.140129, mean_q: 2.739718\n",
      "   662/50000: episode: 128, duration: 0.075s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 1.138329, mae: 1.328580, mean_q: 2.856389\n",
      "   665/50000: episode: 129, duration: 0.095s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.333 [0.000, 3.000],  loss: 1.508392, mae: 1.244982, mean_q: 2.624392\n",
      "   668/50000: episode: 130, duration: 0.094s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 23.117308, mae: 1.294074, mean_q: 2.331071\n",
      "   682/50000: episode: 131, duration: 0.364s, episode steps:  14, steps per second:  38, episode reward:  1.300, mean reward:  0.093 [ 0.000,  0.100], mean action: 1.071 [0.000, 3.000],  loss: 3.941080, mae: 1.266425, mean_q: 2.796253\n",
      "   688/50000: episode: 132, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 21.091692, mae: 1.333414, mean_q: 2.672949\n",
      "   690/50000: episode: 133, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 1.100908, mae: 1.348930, mean_q: 2.988494\n",
      "   697/50000: episode: 134, duration: 0.190s, episode steps:   7, steps per second:  37, episode reward:  0.600, mean reward:  0.086 [ 0.000,  0.100], mean action: 1.429 [0.000, 3.000],  loss: 1.727140, mae: 1.434632, mean_q: 3.102979\n",
      "   700/50000: episode: 135, duration: 0.095s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.333 [0.000, 3.000],  loss: 0.899434, mae: 1.349539, mean_q: 2.914526\n",
      "   702/50000: episode: 136, duration: 0.075s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 11.963875, mae: 1.616798, mean_q: 3.057102\n",
      "   707/50000: episode: 137, duration: 0.141s, episode steps:   5, steps per second:  35, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 1.600 [1.000, 3.000],  loss: 8.067538, mae: 1.429381, mean_q: 2.939699\n",
      "   715/50000: episode: 138, duration: 0.212s, episode steps:   8, steps per second:  38, episode reward:  0.700, mean reward:  0.088 [ 0.000,  0.100], mean action: 0.500 [0.000, 2.000],  loss: 9.446808, mae: 1.395837, mean_q: 2.883558\n",
      "   721/50000: episode: 139, duration: 0.162s, episode steps:   6, steps per second:  37, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 1.167 [0.000, 2.000],  loss: 21.135458, mae: 1.488095, mean_q: 2.871142\n",
      "   723/50000: episode: 140, duration: 0.074s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 1.379179, mae: 1.329728, mean_q: 3.128612\n",
      "   725/50000: episode: 141, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 0.546047, mae: 1.370755, mean_q: 3.290098\n",
      "   729/50000: episode: 142, duration: 0.117s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 1.006784, mae: 1.499894, mean_q: 3.366030\n",
      "   734/50000: episode: 143, duration: 0.140s, episode steps:   5, steps per second:  36, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 2.400 [1.000, 3.000],  loss: 17.562466, mae: 1.505382, mean_q: 3.236488\n",
      "   742/50000: episode: 144, duration: 0.218s, episode steps:   8, steps per second:  37, episode reward:  0.800, mean reward:  0.100 [ 0.000,  0.200], mean action: 1.375 [0.000, 3.000],  loss: 3.511305, mae: 1.573229, mean_q: 3.577731\n",
      "   746/50000: episode: 145, duration: 0.117s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.250 [1.000, 2.000],  loss: 2.871809, mae: 1.706995, mean_q: 3.747684\n",
      "   749/50000: episode: 146, duration: 0.095s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.667 [1.000, 2.000],  loss: 0.534979, mae: 1.753102, mean_q: 3.780155\n",
      "   751/50000: episode: 147, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 2.222603, mae: 1.830595, mean_q: 4.102765\n",
      "   754/50000: episode: 148, duration: 0.096s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.667 [1.000, 2.000],  loss: 4.504922, mae: 1.811165, mean_q: 3.912661\n",
      "   759/50000: episode: 149, duration: 0.141s, episode steps:   5, steps per second:  35, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 1.200 [0.000, 2.000],  loss: 23.560131, mae: 1.833966, mean_q: 4.189804\n",
      "   762/50000: episode: 150, duration: 0.097s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 5.331642, mae: 1.925098, mean_q: 4.915939\n",
      "   765/50000: episode: 151, duration: 0.095s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.667 [1.000, 2.000],  loss: 6.524368, mae: 1.833316, mean_q: 4.646895\n",
      "   769/50000: episode: 152, duration: 0.119s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.250 [0.000, 3.000],  loss: 15.292010, mae: 1.992209, mean_q: 4.990479\n",
      "   774/50000: episode: 153, duration: 0.141s, episode steps:   5, steps per second:  35, episode reward:  0.500, mean reward:  0.100 [ 0.000,  0.200], mean action: 2.000 [1.000, 3.000],  loss: 13.068796, mae: 2.137698, mean_q: 4.981077\n",
      "   776/50000: episode: 154, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 14.287258, mae: 2.400220, mean_q: 5.880303\n",
      "   783/50000: episode: 155, duration: 0.186s, episode steps:   7, steps per second:  38, episode reward:  0.600, mean reward:  0.086 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 16.983309, mae: 2.386628, mean_q: 5.284424\n",
      "   785/50000: episode: 156, duration: 0.074s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 2.471267, mae: 2.701351, mean_q: 6.298455\n",
      "   790/50000: episode: 157, duration: 0.139s, episode steps:   5, steps per second:  36, episode reward:  0.500, mean reward:  0.100 [ 0.000,  0.200], mean action: 1.000 [0.000, 2.000],  loss: 14.841168, mae: 2.392167, mean_q: 5.829417\n",
      "   792/50000: episode: 158, duration: 0.076s, episode steps:   2, steps per second:  26, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 6.679029, mae: 2.388651, mean_q: 5.640560\n",
      "   794/50000: episode: 159, duration: 0.074s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 4.446614, mae: 2.198016, mean_q: 5.054470\n",
      "   802/50000: episode: 160, duration: 0.210s, episode steps:   8, steps per second:  38, episode reward:  0.800, mean reward:  0.100 [ 0.000,  0.200], mean action: 1.000 [1.000, 1.000],  loss: 4.883426, mae: 2.297819, mean_q: 5.523940\n",
      "   804/50000: episode: 161, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.000 [1.000, 1.000],  loss: 186.803253, mae: 2.752136, mean_q: 5.786547\n",
      "   810/50000: episode: 162, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 21.839767, mae: 2.700943, mean_q: 6.273616\n",
      "   818/50000: episode: 163, duration: 0.212s, episode steps:   8, steps per second:  38, episode reward:  0.700, mean reward:  0.088 [ 0.000,  0.100], mean action: 1.125 [0.000, 3.000],  loss: 14.768322, mae: 2.650723, mean_q: 6.379206\n",
      "   821/50000: episode: 164, duration: 0.099s, episode steps:   3, steps per second:  30, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.333 [1.000, 2.000],  loss: 50.172379, mae: 3.188960, mean_q: 6.314174\n",
      "   823/50000: episode: 165, duration: 0.074s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 7.538849, mae: 2.838045, mean_q: 6.447154\n",
      "   828/50000: episode: 166, duration: 0.142s, episode steps:   5, steps per second:  35, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 2.000 [0.000, 3.000],  loss: 6.356299, mae: 2.818339, mean_q: 6.414927\n",
      "   832/50000: episode: 167, duration: 0.117s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.250 [1.000, 2.000],  loss: 25.831654, mae: 2.747212, mean_q: 6.369680\n",
      "   835/50000: episode: 168, duration: 0.097s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.667 [0.000, 3.000],  loss: 4.906663, mae: 2.829273, mean_q: 6.429880\n",
      "   840/50000: episode: 169, duration: 0.139s, episode steps:   5, steps per second:  36, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 2.400 [0.000, 3.000],  loss: 4.231526, mae: 2.716652, mean_q: 6.564649\n",
      "   846/50000: episode: 170, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 2.333 [1.000, 3.000],  loss: 25.084837, mae: 2.891077, mean_q: 6.459382\n",
      "   850/50000: episode: 171, duration: 0.117s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.750 [1.000, 3.000],  loss: 34.977989, mae: 2.998710, mean_q: 6.105720\n",
      "   853/50000: episode: 172, duration: 0.095s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 2.000 [0.000, 3.000],  loss: 6.353388, mae: 2.819216, mean_q: 5.975130\n",
      "   855/50000: episode: 173, duration: 0.073s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 2.500 [2.000, 3.000],  loss: 10.075794, mae: 2.683942, mean_q: 6.053536\n",
      "   857/50000: episode: 174, duration: 0.073s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 7.045667, mae: 2.890978, mean_q: 5.696350\n",
      "   859/50000: episode: 175, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 222.294327, mae: 3.161932, mean_q: 5.916649\n",
      "   861/50000: episode: 176, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 3.156532, mae: 2.870683, mean_q: 6.026130\n",
      "   864/50000: episode: 177, duration: 0.096s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.667 [0.000, 3.000],  loss: 17.747488, mae: 2.973053, mean_q: 6.292885\n",
      "   881/50000: episode: 178, duration: 0.415s, episode steps:  17, steps per second:  41, episode reward:  1.600, mean reward:  0.094 [ 0.000,  0.100], mean action: 1.647 [0.000, 3.000],  loss: 94.876015, mae: 3.509424, mean_q: 6.929897\n",
      "   884/50000: episode: 179, duration: 0.095s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 80.670952, mae: 3.874312, mean_q: 8.215383\n",
      "   897/50000: episode: 180, duration: 0.332s, episode steps:  13, steps per second:  39, episode reward:  1.200, mean reward:  0.092 [ 0.000,  0.100], mean action: 0.923 [0.000, 3.000],  loss: 34.913136, mae: 3.596081, mean_q: 7.089575\n",
      "   899/50000: episode: 181, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 43.278229, mae: 3.726143, mean_q: 6.577491\n",
      "   901/50000: episode: 182, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 36.046505, mae: 3.397610, mean_q: 6.389240\n",
      "   904/50000: episode: 183, duration: 0.107s, episode steps:   3, steps per second:  28, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 276.702118, mae: 3.892043, mean_q: 6.389616\n",
      "   908/50000: episode: 184, duration: 0.119s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.250 [1.000, 2.000],  loss: 73.179680, mae: 3.666993, mean_q: 6.508082\n",
      "   910/50000: episode: 185, duration: 0.074s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 12.314193, mae: 3.517288, mean_q: 6.141383\n",
      "   912/50000: episode: 186, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 49.627686, mae: 3.535814, mean_q: 6.192049\n",
      "   915/50000: episode: 187, duration: 0.099s, episode steps:   3, steps per second:  30, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 12.810028, mae: 3.543342, mean_q: 6.392342\n",
      "   919/50000: episode: 188, duration: 0.118s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 38.552727, mae: 3.512239, mean_q: 6.475801\n",
      "   921/50000: episode: 189, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 32.087791, mae: 4.017510, mean_q: 6.377644\n",
      "   929/50000: episode: 190, duration: 0.214s, episode steps:   8, steps per second:  37, episode reward:  0.700, mean reward:  0.088 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 129.556061, mae: 4.135057, mean_q: 6.450865\n",
      "   937/50000: episode: 191, duration: 0.209s, episode steps:   8, steps per second:  38, episode reward:  0.700, mean reward:  0.088 [ 0.000,  0.100], mean action: 2.125 [0.000, 3.000],  loss: 18.730881, mae: 4.081249, mean_q: 7.649745\n",
      "   945/50000: episode: 192, duration: 0.210s, episode steps:   8, steps per second:  38, episode reward:  0.700, mean reward:  0.088 [ 0.000,  0.100], mean action: 1.375 [0.000, 3.000],  loss: 87.385201, mae: 4.253539, mean_q: 7.663107\n",
      "   948/50000: episode: 193, duration: 0.095s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 2.000 [0.000, 3.000],  loss: 8.827374, mae: 4.153378, mean_q: 8.233169\n",
      "   951/50000: episode: 194, duration: 0.096s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 14.175270, mae: 4.042645, mean_q: 8.110087\n",
      "   956/50000: episode: 195, duration: 0.144s, episode steps:   5, steps per second:  35, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 1.200 [0.000, 2.000],  loss: 17.096287, mae: 4.243989, mean_q: 9.191418\n",
      "   966/50000: episode: 196, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.900, mean reward:  0.090 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 167.713898, mae: 4.000009, mean_q: 8.394820\n",
      "   968/50000: episode: 197, duration: 0.071s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 406.419067, mae: 4.551777, mean_q: 8.331455\n",
      "   972/50000: episode: 198, duration: 0.118s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.000 [0.000, 3.000],  loss: 66.167313, mae: 4.702290, mean_q: 8.263743\n",
      "   976/50000: episode: 199, duration: 0.119s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.750 [1.000, 3.000],  loss: 36.775635, mae: 4.635787, mean_q: 8.286141\n",
      "   984/50000: episode: 200, duration: 0.211s, episode steps:   8, steps per second:  38, episode reward:  0.700, mean reward:  0.088 [ 0.000,  0.100], mean action: 1.125 [0.000, 3.000],  loss: 177.759338, mae: 4.986032, mean_q: 8.977878\n",
      "   999/50000: episode: 201, duration: 0.374s, episode steps:  15, steps per second:  40, episode reward:  1.400, mean reward:  0.093 [ 0.000,  0.100], mean action: 2.333 [0.000, 3.000],  loss: 71.513336, mae: 4.526787, mean_q: 9.783264\n",
      "  1005/50000: episode: 202, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.600, mean reward:  0.100 [ 0.000,  0.200], mean action: 1.500 [0.000, 3.000],  loss: 64.139397, mae: 4.501319, mean_q: 8.310731\n",
      "  1008/50000: episode: 203, duration: 0.095s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.333 [0.000, 3.000],  loss: 16.706152, mae: 4.177936, mean_q: 8.988271\n",
      "  1013/50000: episode: 204, duration: 0.168s, episode steps:   5, steps per second:  30, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 2.200 [1.000, 3.000],  loss: 109.787155, mae: 4.312209, mean_q: 8.280867\n",
      "  1017/50000: episode: 205, duration: 0.126s, episode steps:   4, steps per second:  32, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.250 [0.000, 2.000],  loss: 37.347870, mae: 4.711899, mean_q: 9.521388\n",
      "  1020/50000: episode: 206, duration: 0.098s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.667 [0.000, 3.000],  loss: 38.975666, mae: 4.494350, mean_q: 9.361044\n",
      "  1028/50000: episode: 207, duration: 0.214s, episode steps:   8, steps per second:  37, episode reward:  0.800, mean reward:  0.100 [ 0.000,  0.200], mean action: 2.250 [0.000, 3.000],  loss: 30.224388, mae: 4.603422, mean_q: 8.374498\n",
      "  1040/50000: episode: 208, duration: 0.305s, episode steps:  12, steps per second:  39, episode reward:  1.100, mean reward:  0.092 [ 0.000,  0.100], mean action: 2.167 [0.000, 3.000],  loss: 35.425938, mae: 4.427302, mean_q: 7.510592\n",
      "  1042/50000: episode: 209, duration: 0.077s, episode steps:   2, steps per second:  26, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 1047.023315, mae: 5.885802, mean_q: 7.253653\n",
      "  1044/50000: episode: 210, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 16.047245, mae: 3.972104, mean_q: 6.549731\n",
      "  1058/50000: episode: 211, duration: 0.348s, episode steps:  14, steps per second:  40, episode reward:  1.300, mean reward:  0.093 [ 0.000,  0.100], mean action: 2.214 [1.000, 3.000],  loss: 25.885771, mae: 3.678022, mean_q: 6.649108\n",
      "  1063/50000: episode: 212, duration: 0.141s, episode steps:   5, steps per second:  35, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 16.349901, mae: 3.547785, mean_q: 6.927460\n",
      "  1066/50000: episode: 213, duration: 0.095s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.667 [0.000, 3.000],  loss: 69.545113, mae: 4.348813, mean_q: 7.378039\n",
      "  1068/50000: episode: 214, duration: 0.074s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 46.796959, mae: 4.256533, mean_q: 7.558644\n",
      "  1069/50000: episode: 215, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.000 [1.000, 1.000],  loss: 15.521816, mae: 3.631896, mean_q: 6.602677\n",
      "  1075/50000: episode: 216, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 10.524140, mae: 3.746875, mean_q: 6.954769\n",
      "  1079/50000: episode: 217, duration: 0.117s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 2.250 [1.000, 3.000],  loss: 21.982143, mae: 3.418108, mean_q: 6.446352\n",
      "  1081/50000: episode: 218, duration: 0.075s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 22.460464, mae: 3.760358, mean_q: 6.562609\n",
      "  1098/50000: episode: 219, duration: 0.421s, episode steps:  17, steps per second:  40, episode reward:  1.600, mean reward:  0.094 [ 0.000,  0.100], mean action: 2.176 [0.000, 3.000],  loss: 46.175426, mae: 3.996166, mean_q: 7.843335\n",
      "  1107/50000: episode: 220, duration: 0.241s, episode steps:   9, steps per second:  37, episode reward:  0.900, mean reward:  0.100 [ 0.000,  0.200], mean action: 2.222 [0.000, 3.000],  loss: 27.452797, mae: 4.070646, mean_q: 7.357632\n",
      "  1109/50000: episode: 221, duration: 0.074s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 87.033630, mae: 4.036294, mean_q: 7.423195\n",
      "  1112/50000: episode: 222, duration: 0.095s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 2.000 [0.000, 3.000],  loss: 52.759777, mae: 4.521351, mean_q: 7.410002\n",
      "  1115/50000: episode: 223, duration: 0.095s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 11.609422, mae: 3.968135, mean_q: 7.740376\n",
      "  1117/50000: episode: 224, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 28.987192, mae: 4.183953, mean_q: 7.026608\n",
      "  1124/50000: episode: 225, duration: 0.197s, episode steps:   7, steps per second:  35, episode reward:  0.600, mean reward:  0.086 [ 0.000,  0.100], mean action: 2.000 [0.000, 3.000],  loss: 16.912516, mae: 3.873787, mean_q: 6.261012\n",
      "  1126/50000: episode: 226, duration: 0.076s, episode steps:   2, steps per second:  26, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 23.156933, mae: 3.934029, mean_q: 6.232277\n",
      "  1135/50000: episode: 227, duration: 0.234s, episode steps:   9, steps per second:  39, episode reward:  0.800, mean reward:  0.089 [ 0.000,  0.100], mean action: 1.444 [0.000, 3.000],  loss: 20.747002, mae: 3.855213, mean_q: 6.200928\n",
      "  1138/50000: episode: 228, duration: 0.097s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 6.387675, mae: 3.782852, mean_q: 6.605387\n",
      "  1144/50000: episode: 229, duration: 0.163s, episode steps:   6, steps per second:  37, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 1.833 [0.000, 3.000],  loss: 21.689440, mae: 3.779642, mean_q: 6.742178\n",
      "  1147/50000: episode: 230, duration: 0.096s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.667 [0.000, 3.000],  loss: 15.537982, mae: 3.529067, mean_q: 6.637832\n",
      "  1155/50000: episode: 231, duration: 0.211s, episode steps:   8, steps per second:  38, episode reward:  0.700, mean reward:  0.088 [ 0.000,  0.100], mean action: 1.625 [0.000, 3.000],  loss: 24.913464, mae: 3.986758, mean_q: 6.697618\n",
      "  1162/50000: episode: 232, duration: 0.185s, episode steps:   7, steps per second:  38, episode reward:  0.600, mean reward:  0.086 [ 0.000,  0.100], mean action: 2.143 [0.000, 3.000],  loss: 19.865891, mae: 4.399854, mean_q: 7.383183\n",
      "  1164/50000: episode: 233, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 18.041128, mae: 4.405245, mean_q: 7.476759\n",
      "  1176/50000: episode: 234, duration: 0.309s, episode steps:  12, steps per second:  39, episode reward: 34.100, mean reward:  2.842 [ 0.100, 33.000], mean action: 1.417 [0.000, 3.000],  loss: 28.358011, mae: 4.479826, mean_q: 7.170669\n",
      "  1178/50000: episode: 235, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 35.248299, mae: 4.288816, mean_q: 7.002282\n",
      "  1183/50000: episode: 236, duration: 0.148s, episode steps:   5, steps per second:  34, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 6.670424, mae: 3.828788, mean_q: 6.804288\n",
      "  1190/50000: episode: 237, duration: 0.190s, episode steps:   7, steps per second:  37, episode reward:  0.600, mean reward:  0.086 [ 0.000,  0.100], mean action: 2.143 [0.000, 3.000],  loss: 15.895563, mae: 4.205348, mean_q: 6.669721\n",
      "  1195/50000: episode: 238, duration: 0.148s, episode steps:   5, steps per second:  34, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 1.800 [0.000, 3.000],  loss: 16.498386, mae: 4.178207, mean_q: 6.971717\n",
      "  1198/50000: episode: 239, duration: 0.095s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.333 [0.000, 3.000],  loss: 12.747074, mae: 4.009527, mean_q: 6.368135\n",
      "  1203/50000: episode: 240, duration: 0.140s, episode steps:   5, steps per second:  36, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 2.000 [0.000, 3.000],  loss: 23.186317, mae: 3.739171, mean_q: 5.801456\n",
      "  1206/50000: episode: 241, duration: 0.096s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 48.307705, mae: 4.082540, mean_q: 5.753091\n",
      "  1216/50000: episode: 242, duration: 0.256s, episode steps:  10, steps per second:  39, episode reward:  1.000, mean reward:  0.100 [ 0.000,  0.200], mean action: 0.900 [0.000, 2.000],  loss: 16.242569, mae: 3.714329, mean_q: 5.848875\n",
      "  1218/50000: episode: 243, duration: 0.073s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 47.315239, mae: 4.308886, mean_q: 6.348335\n",
      "  1220/50000: episode: 244, duration: 0.071s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 7.706469, mae: 3.936625, mean_q: 5.742485\n",
      "  1234/50000: episode: 245, duration: 0.345s, episode steps:  14, steps per second:  41, episode reward:  1.300, mean reward:  0.093 [ 0.000,  0.100], mean action: 2.214 [0.000, 3.000],  loss: 34.641563, mae: 3.970563, mean_q: 6.022358\n",
      "  1236/50000: episode: 246, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 36.786308, mae: 4.167770, mean_q: 6.344273\n",
      "  1238/50000: episode: 247, duration: 0.073s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 307.888397, mae: 4.825748, mean_q: 6.714747\n",
      "  1242/50000: episode: 248, duration: 0.118s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 2.000 [0.000, 3.000],  loss: 15.368444, mae: 3.887465, mean_q: 6.935826\n",
      "  1245/50000: episode: 249, duration: 0.095s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 15.335700, mae: 4.141160, mean_q: 7.146640\n",
      "  1250/50000: episode: 250, duration: 0.142s, episode steps:   5, steps per second:  35, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 2.400 [1.000, 3.000],  loss: 86.004311, mae: 4.578558, mean_q: 7.439491\n",
      "  1253/50000: episode: 251, duration: 0.095s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 2.000 [0.000, 3.000],  loss: 15.770420, mae: 4.366889, mean_q: 7.032106\n",
      "  1261/50000: episode: 252, duration: 0.209s, episode steps:   8, steps per second:  38, episode reward:  0.900, mean reward:  0.113 [ 0.000,  0.200], mean action: 1.125 [0.000, 3.000],  loss: 12.505560, mae: 4.457192, mean_q: 7.191559\n",
      "  1266/50000: episode: 253, duration: 0.142s, episode steps:   5, steps per second:  35, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 0.400 [0.000, 1.000],  loss: 14.390279, mae: 4.261710, mean_q: 6.444513\n",
      "  1268/50000: episode: 254, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 12.752731, mae: 4.319162, mean_q: 7.025608\n",
      "  1270/50000: episode: 255, duration: 0.074s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 10.964540, mae: 3.949873, mean_q: 6.157197\n",
      "  1274/50000: episode: 256, duration: 0.117s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.250 [0.000, 2.000],  loss: 19.350151, mae: 3.952899, mean_q: 6.033576\n",
      "  1280/50000: episode: 257, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 0.667 [0.000, 3.000],  loss: 44.621761, mae: 3.991730, mean_q: 6.291169\n",
      "  1287/50000: episode: 258, duration: 0.185s, episode steps:   7, steps per second:  38, episode reward:  0.600, mean reward:  0.086 [ 0.000,  0.100], mean action: 1.714 [0.000, 3.000],  loss: 22.671112, mae: 4.079320, mean_q: 6.684988\n",
      "  1289/50000: episode: 259, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 9.808577, mae: 3.896169, mean_q: 6.722719\n",
      "  1291/50000: episode: 260, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 10.113982, mae: 3.847790, mean_q: 6.580359\n",
      "  1295/50000: episode: 261, duration: 0.119s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 75.120628, mae: 4.071192, mean_q: 6.454175\n",
      "  1301/50000: episode: 262, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 2.167 [2.000, 3.000],  loss: 16.902636, mae: 3.968918, mean_q: 6.737368\n",
      "  1306/50000: episode: 263, duration: 0.140s, episode steps:   5, steps per second:  36, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 2.200 [1.000, 3.000],  loss: 6.318582, mae: 3.829342, mean_q: 6.648233\n",
      "  1310/50000: episode: 264, duration: 0.117s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 17.143707, mae: 3.550165, mean_q: 6.580350\n",
      "  1312/50000: episode: 265, duration: 0.075s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 28.815968, mae: 3.460660, mean_q: 5.798231\n",
      "  1325/50000: episode: 266, duration: 0.322s, episode steps:  13, steps per second:  40, episode reward: 34.200, mean reward:  2.631 [ 0.100, 33.000], mean action: 2.077 [0.000, 3.000],  loss: 60.494144, mae: 3.601678, mean_q: 5.943400\n",
      "  1328/50000: episode: 267, duration: 0.095s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 2.000 [0.000, 3.000],  loss: 6.753790, mae: 3.545606, mean_q: 6.555364\n",
      "  1332/50000: episode: 268, duration: 0.118s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.250 [0.000, 3.000],  loss: 105.445068, mae: 3.777191, mean_q: 6.443238\n",
      "  1335/50000: episode: 269, duration: 0.095s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 256.601349, mae: 4.612898, mean_q: 6.054680\n",
      "  1339/50000: episode: 270, duration: 0.121s, episode steps:   4, steps per second:  33, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.250 [0.000, 3.000],  loss: 98.503113, mae: 3.728680, mean_q: 6.114676\n",
      "  1341/50000: episode: 271, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 22.870148, mae: 3.629962, mean_q: 6.675727\n",
      "  1353/50000: episode: 272, duration: 0.309s, episode steps:  12, steps per second:  39, episode reward:  1.100, mean reward:  0.092 [ 0.000,  0.100], mean action: 2.083 [1.000, 3.000],  loss: 46.707123, mae: 3.520018, mean_q: 5.488316\n",
      "  1364/50000: episode: 273, duration: 0.284s, episode steps:  11, steps per second:  39, episode reward: 34.100, mean reward:  3.100 [ 0.100, 33.000], mean action: 0.909 [0.000, 2.000],  loss: 49.718445, mae: 3.369555, mean_q: 5.318697\n",
      "  1368/50000: episode: 274, duration: 0.118s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.750 [0.000, 3.000],  loss: 61.550392, mae: 3.767620, mean_q: 5.850523\n",
      "  1370/50000: episode: 275, duration: 0.071s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 6.628297, mae: 3.434499, mean_q: 5.299292\n",
      "  1373/50000: episode: 276, duration: 0.095s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.667 [1.000, 2.000],  loss: 9.608685, mae: 3.668950, mean_q: 5.737010\n",
      "  1381/50000: episode: 277, duration: 0.210s, episode steps:   8, steps per second:  38, episode reward:  0.700, mean reward:  0.088 [ 0.000,  0.100], mean action: 0.750 [0.000, 3.000],  loss: 65.834641, mae: 3.856003, mean_q: 5.530955\n",
      "  1386/50000: episode: 278, duration: 0.143s, episode steps:   5, steps per second:  35, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 9.944128, mae: 3.237752, mean_q: 5.479537\n",
      "  1390/50000: episode: 279, duration: 0.118s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 129.018326, mae: 3.483546, mean_q: 5.018176\n",
      "  1396/50000: episode: 280, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward: 33.600, mean reward:  5.600 [ 0.100, 33.000], mean action: 1.833 [0.000, 3.000],  loss: 90.853241, mae: 3.286695, mean_q: 5.055626\n",
      "  1401/50000: episode: 281, duration: 0.143s, episode steps:   5, steps per second:  35, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 1.400 [0.000, 2.000],  loss: 8.857982, mae: 3.023134, mean_q: 5.141225\n",
      "  1407/50000: episode: 282, duration: 0.165s, episode steps:   6, steps per second:  36, episode reward:  0.600, mean reward:  0.100 [ 0.000,  0.200], mean action: 2.167 [0.000, 3.000],  loss: 68.294159, mae: 3.575244, mean_q: 5.465239\n",
      "  1410/50000: episode: 283, duration: 0.100s, episode steps:   3, steps per second:  30, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 3.876988, mae: 2.958520, mean_q: 5.202572\n",
      "  1413/50000: episode: 284, duration: 0.095s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 18.739904, mae: 3.223975, mean_q: 4.800776\n",
      "  1417/50000: episode: 285, duration: 0.118s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.750 [0.000, 3.000],  loss: 24.189953, mae: 3.098381, mean_q: 4.797183\n",
      "  1419/50000: episode: 286, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 10.795001, mae: 3.077072, mean_q: 5.185421\n",
      "  1423/50000: episode: 287, duration: 0.118s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.750 [1.000, 3.000],  loss: 39.702721, mae: 3.103248, mean_q: 4.740092\n",
      "  1430/50000: episode: 288, duration: 0.185s, episode steps:   7, steps per second:  38, episode reward:  0.600, mean reward:  0.086 [ 0.000,  0.100], mean action: 2.000 [0.000, 3.000],  loss: 40.773811, mae: 3.025203, mean_q: 5.144578\n",
      "  1451/50000: episode: 289, duration: 0.508s, episode steps:  21, steps per second:  41, episode reward:  2.000, mean reward:  0.095 [ 0.000,  0.100], mean action: 1.714 [0.000, 3.000],  loss: 101.923943, mae: 3.313766, mean_q: 5.187214\n",
      "  1466/50000: episode: 290, duration: 0.369s, episode steps:  15, steps per second:  41, episode reward:  1.400, mean reward:  0.093 [ 0.000,  0.100], mean action: 1.600 [0.000, 3.000],  loss: 107.276085, mae: 3.633324, mean_q: 5.733983\n",
      "  1468/50000: episode: 291, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 19.822025, mae: 2.743980, mean_q: 4.862991\n",
      "  1474/50000: episode: 292, duration: 0.164s, episode steps:   6, steps per second:  37, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 9.640287, mae: 2.813269, mean_q: 5.422901\n",
      "  1491/50000: episode: 293, duration: 0.422s, episode steps:  17, steps per second:  40, episode reward: 34.700, mean reward:  2.041 [ 0.100, 33.000], mean action: 1.882 [0.000, 3.000],  loss: 136.533127, mae: 3.431125, mean_q: 5.785760\n",
      "  1495/50000: episode: 294, duration: 0.121s, episode steps:   4, steps per second:  33, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.250 [1.000, 2.000],  loss: 97.602554, mae: 3.626510, mean_q: 5.962786\n",
      "  1497/50000: episode: 295, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 76.356010, mae: 4.091331, mean_q: 6.127047\n",
      "  1499/50000: episode: 296, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 164.751053, mae: 4.100060, mean_q: 6.690578\n",
      "  1508/50000: episode: 297, duration: 0.237s, episode steps:   9, steps per second:  38, episode reward:  0.800, mean reward:  0.089 [ 0.000,  0.100], mean action: 2.111 [1.000, 3.000],  loss: 208.164246, mae: 3.845784, mean_q: 6.309474\n",
      "  1510/50000: episode: 298, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 7.755285, mae: 3.231145, mean_q: 5.702370\n",
      "  1512/50000: episode: 299, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 127.491196, mae: 3.116007, mean_q: 6.051559\n",
      "  1515/50000: episode: 300, duration: 0.096s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.667 [0.000, 3.000],  loss: 438.960175, mae: 3.752445, mean_q: 5.992087\n",
      "  1520/50000: episode: 301, duration: 0.141s, episode steps:   5, steps per second:  35, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 12.328191, mae: 2.773751, mean_q: 5.323368\n",
      "  1529/50000: episode: 302, duration: 0.235s, episode steps:   9, steps per second:  38, episode reward:  0.800, mean reward:  0.089 [ 0.000,  0.100], mean action: 2.333 [1.000, 3.000],  loss: 218.274521, mae: 3.523550, mean_q: 5.346348\n",
      "  1531/50000: episode: 303, duration: 0.074s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 4.833365, mae: 2.822580, mean_q: 5.051605\n",
      "  1537/50000: episode: 304, duration: 0.168s, episode steps:   6, steps per second:  36, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 1.167 [0.000, 3.000],  loss: 52.031902, mae: 3.335291, mean_q: 5.539663\n",
      "  1540/50000: episode: 305, duration: 0.095s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 2.333 [2.000, 3.000],  loss: 350.825348, mae: 3.564755, mean_q: 5.250266\n",
      "  1544/50000: episode: 306, duration: 0.118s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.750 [1.000, 2.000],  loss: 328.378540, mae: 3.729807, mean_q: 5.416214\n",
      "  1548/50000: episode: 307, duration: 0.119s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 234.660782, mae: 3.625075, mean_q: 5.888586\n",
      "  1551/50000: episode: 308, duration: 0.097s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.667 [0.000, 3.000],  loss: 15.276324, mae: 3.113935, mean_q: 5.518432\n",
      "  1557/50000: episode: 309, duration: 0.166s, episode steps:   6, steps per second:  36, episode reward:  0.500, mean reward:  0.083 [ 0.000,  0.100], mean action: 1.333 [0.000, 2.000],  loss: 9.471596, mae: 3.032034, mean_q: 5.966432\n",
      "  1560/50000: episode: 310, duration: 0.099s, episode steps:   3, steps per second:  30, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.667 [1.000, 2.000],  loss: 492.665161, mae: 4.315320, mean_q: 5.880539\n",
      "  1565/50000: episode: 311, duration: 0.146s, episode steps:   5, steps per second:  34, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 2.200 [1.000, 3.000],  loss: 16.526976, mae: 2.828603, mean_q: 5.130680\n",
      "  1574/50000: episode: 312, duration: 0.255s, episode steps:   9, steps per second:  35, episode reward:  0.900, mean reward:  0.100 [ 0.000,  0.200], mean action: 1.111 [0.000, 2.000],  loss: 38.812332, mae: 2.799035, mean_q: 5.224852\n",
      "  1576/50000: episode: 313, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 5.458035, mae: 2.866300, mean_q: 5.176454\n",
      "  1579/50000: episode: 314, duration: 0.094s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 2.000 [0.000, 3.000],  loss: 337.785187, mae: 3.587344, mean_q: 5.442596\n",
      "  1581/50000: episode: 315, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 13.333948, mae: 2.905932, mean_q: 5.046047\n",
      "  1583/50000: episode: 316, duration: 0.076s, episode steps:   2, steps per second:  26, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 121.206238, mae: 2.977527, mean_q: 5.252343\n",
      "  1588/50000: episode: 317, duration: 0.143s, episode steps:   5, steps per second:  35, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 147.106903, mae: 2.788149, mean_q: 4.489724\n",
      "  1591/50000: episode: 318, duration: 0.096s, episode steps:   3, steps per second:  31, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.333 [0.000, 3.000],  loss: 20.750910, mae: 2.659055, mean_q: 4.520353\n",
      "  1602/50000: episode: 319, duration: 0.277s, episode steps:  11, steps per second:  40, episode reward:  1.000, mean reward:  0.091 [ 0.000,  0.100], mean action: 1.909 [0.000, 3.000],  loss: 356.864624, mae: 3.599114, mean_q: 5.284202\n",
      "  1604/50000: episode: 320, duration: 0.074s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 7.799760, mae: 3.073607, mean_q: 5.237645\n",
      "  1611/50000: episode: 321, duration: 0.210s, episode steps:   7, steps per second:  33, episode reward:  0.600, mean reward:  0.086 [ 0.000,  0.100], mean action: 1.571 [0.000, 3.000],  loss: 14.379830, mae: 2.562784, mean_q: 4.814457\n",
      "  1615/50000: episode: 322, duration: 0.122s, episode steps:   4, steps per second:  33, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 0.750 [0.000, 3.000],  loss: 69.662895, mae: 2.397294, mean_q: 4.075200\n",
      "  1617/50000: episode: 323, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 1133.393433, mae: 4.310436, mean_q: 4.206742\n",
      "  1619/50000: episode: 324, duration: 0.074s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 4.588097, mae: 2.003727, mean_q: 4.022623\n",
      "  1621/50000: episode: 325, duration: 0.077s, episode steps:   2, steps per second:  26, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 1737.438721, mae: 5.133039, mean_q: 5.009848\n",
      "  1623/50000: episode: 326, duration: 0.082s, episode steps:   2, steps per second:  24, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 1232.054077, mae: 4.383866, mean_q: 5.223526\n",
      "  1645/50000: episode: 327, duration: 0.536s, episode steps:  22, steps per second:  41, episode reward: 35.100, mean reward:  1.595 [ 0.100, 33.000], mean action: 1.909 [0.000, 3.000],  loss: 152.065933, mae: 3.286768, mean_q: 5.310103\n",
      "  1655/50000: episode: 328, duration: 0.258s, episode steps:  10, steps per second:  39, episode reward:  0.900, mean reward:  0.090 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 241.696747, mae: 3.170045, mean_q: 5.359023\n",
      "  1658/50000: episode: 329, duration: 0.099s, episode steps:   3, steps per second:  30, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 104.861969, mae: 3.149853, mean_q: 6.006117\n",
      "  1661/50000: episode: 330, duration: 0.099s, episode steps:   3, steps per second:  30, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.667 [0.000, 3.000],  loss: 110.313942, mae: 2.892661, mean_q: 5.435581\n",
      "  1665/50000: episode: 331, duration: 0.127s, episode steps:   4, steps per second:  32, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.750 [1.000, 3.000],  loss: 712.183350, mae: 4.380145, mean_q: 5.857254\n",
      "  1667/50000: episode: 332, duration: 0.074s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 706.921814, mae: 4.461878, mean_q: 5.292443\n",
      "  1672/50000: episode: 333, duration: 0.144s, episode steps:   5, steps per second:  35, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 0.800 [0.000, 2.000],  loss: 592.804016, mae: 3.633815, mean_q: 5.693892\n",
      "  1675/50000: episode: 334, duration: 0.098s, episode steps:   3, steps per second:  30, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 1.000 [0.000, 3.000],  loss: 1351.299438, mae: 4.764590, mean_q: 6.331239\n",
      "  1679/50000: episode: 335, duration: 0.118s, episode steps:   4, steps per second:  34, episode reward:  0.300, mean reward:  0.075 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 26.408564, mae: 3.173021, mean_q: 5.365924\n",
      "  1690/50000: episode: 336, duration: 0.284s, episode steps:  11, steps per second:  39, episode reward:  1.000, mean reward:  0.091 [ 0.000,  0.100], mean action: 1.545 [0.000, 3.000],  loss: 718.304688, mae: 4.335058, mean_q: 5.796817\n",
      "  1692/50000: episode: 337, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 283.338196, mae: 3.442057, mean_q: 5.894763\n",
      "  1697/50000: episode: 338, duration: 0.140s, episode steps:   5, steps per second:  36, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 20.291895, mae: 2.623157, mean_q: 5.909055\n",
      "  1699/50000: episode: 339, duration: 0.097s, episode steps:   2, steps per second:  21, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 3.780955, mae: 2.846470, mean_q: 5.435622\n",
      "  1702/50000: episode: 340, duration: 0.095s, episode steps:   3, steps per second:  32, episode reward:  0.200, mean reward:  0.067 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 6.535976, mae: 2.804080, mean_q: 5.580368\n",
      "  1707/50000: episode: 341, duration: 0.140s, episode steps:   5, steps per second:  36, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 1.400 [0.000, 2.000],  loss: 596.620667, mae: 4.178597, mean_q: 6.007185\n",
      "  1712/50000: episode: 342, duration: 0.140s, episode steps:   5, steps per second:  36, episode reward:  0.400, mean reward:  0.080 [ 0.000,  0.100], mean action: 1.000 [0.000, 2.000],  loss: 326.493744, mae: 3.764421, mean_q: 6.358134\n",
      "  1714/50000: episode: 343, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 10.805204, mae: 2.950648, mean_q: 5.991223\n",
      "  1716/50000: episode: 344, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [0.000, 3.000],  loss: 1226.562744, mae: 4.992588, mean_q: 6.468209\n",
      "  1720/50000: episode: 345, duration: 0.117s, episode steps:   4, steps per second:  34, episode reward:  0.400, mean reward:  0.100 [ 0.000,  0.200], mean action: 1.750 [0.000, 3.000],  loss: 155.009766, mae: 3.742200, mean_q: 5.908144\n",
      "  1730/50000: episode: 346, duration: 0.257s, episode steps:  10, steps per second:  39, episode reward:  0.900, mean reward:  0.090 [ 0.000,  0.100], mean action: 1.600 [0.000, 3.000],  loss: 405.200470, mae: 3.451564, mean_q: 5.627403\n",
      "  1732/50000: episode: 347, duration: 0.076s, episode steps:   2, steps per second:  26, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 1.500 [1.000, 2.000],  loss: 50.741543, mae: 3.557056, mean_q: 6.308952\n",
      "  1734/50000: episode: 348, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 2.000 [2.000, 2.000],  loss: 8.919695, mae: 2.624449, mean_q: 5.110731\n",
      "  1736/50000: episode: 349, duration: 0.073s, episode steps:   2, steps per second:  27, episode reward:  0.100, mean reward:  0.050 [ 0.000,  0.100], mean action: 2.000 [1.000, 3.000],  loss: 25.878304, mae: 3.204189, mean_q: 5.384892\n",
      "done, took 57.191 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc771186f70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model,4)\n",
    "dqn.compile(Adam(lr = 1e-3,clipvalue=1), metrics = ['mae'])\n",
    "dqn.fit(env, nb_steps = 50000, visualize = False, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f4d6d7c-f164-4d81-ad7f-11cdcd73a846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 0.100, steps: 2\n",
      "Episode 2: reward: 1.700, steps: 18\n",
      "Episode 3: reward: 0.100, steps: 2\n",
      "Episode 4: reward: 0.900, steps: 10\n",
      "Episode 5: reward: 0.200, steps: 3\n",
      "Episode 6: reward: 0.200, steps: 3\n",
      "Episode 7: reward: 1.700, steps: 18\n",
      "Episode 8: reward: 34.500, steps: 16\n",
      "Episode 9: reward: 0.800, steps: 9\n",
      "Episode 10: reward: 2.500, steps: 25\n",
      "Episode 11: reward: 3.500, steps: 34\n",
      "Episode 12: reward: 0.300, steps: 4\n",
      "Episode 13: reward: 0.200, steps: 3\n",
      "Episode 14: reward: 0.400, steps: 5\n",
      "Episode 15: reward: 0.400, steps: 5\n",
      "Episode 16: reward: 0.400, steps: 5\n",
      "Episode 17: reward: 0.600, steps: 7\n",
      "Episode 18: reward: 1.900, steps: 19\n",
      "Episode 19: reward: 0.100, steps: 2\n",
      "Episode 20: reward: 0.400, steps: 4\n",
      "Episode 21: reward: 1.400, steps: 15\n",
      "Episode 22: reward: 0.100, steps: 2\n",
      "Episode 23: reward: 0.100, steps: 2\n",
      "Episode 24: reward: 0.900, steps: 10\n",
      "Episode 25: reward: 0.100, steps: 2\n",
      "Episode 26: reward: 1.600, steps: 17\n",
      "Episode 27: reward: 34.100, steps: 12\n",
      "Episode 28: reward: 1.400, steps: 15\n",
      "Episode 29: reward: 0.100, steps: 2\n",
      "Episode 30: reward: 0.100, steps: 2\n",
      "Episode 31: reward: 39.100, steps: 60\n",
      "Episode 32: reward: 0.200, steps: 3\n",
      "Episode 33: reward: 0.100, steps: 2\n",
      "Episode 34: reward: 0.500, steps: 6\n",
      "Episode 35: reward: 36.200, steps: 32\n",
      "Episode 36: reward: 0.900, steps: 10\n",
      "Episode 37: reward: 0.500, steps: 6\n",
      "Episode 38: reward: 2.100, steps: 22\n",
      "Episode 39: reward: 0.100, steps: 2\n",
      "Episode 40: reward: 0.200, steps: 3\n",
      "Episode 41: reward: 0.900, steps: 9\n",
      "Episode 42: reward: 33.000, steps: 1\n",
      "Episode 43: reward: 0.900, steps: 10\n",
      "Episode 44: reward: 0.400, steps: 5\n",
      "Episode 45: reward: 0.200, steps: 3\n",
      "Episode 46: reward: 0.700, steps: 8\n",
      "Episode 47: reward: 1.700, steps: 18\n",
      "Episode 48: reward: 0.500, steps: 6\n",
      "Episode 49: reward: 1.200, steps: 12\n",
      "Episode 50: reward: 2.700, steps: 26\n",
      "Episode 51: reward: 0.300, steps: 4\n",
      "Episode 52: reward: 36.900, steps: 40\n",
      "Episode 53: reward: 0.100, steps: 2\n",
      "Episode 54: reward: 0.300, steps: 4\n",
      "Episode 55: reward: 0.900, steps: 10\n",
      "Episode 56: reward: 1.000, steps: 11\n",
      "Episode 57: reward: 1.000, steps: 11\n",
      "Episode 58: reward: 1.200, steps: 11\n",
      "Episode 59: reward: 0.000, steps: 1\n",
      "Episode 60: reward: 1.200, steps: 13\n",
      "Episode 61: reward: 0.700, steps: 7\n",
      "Episode 62: reward: 0.300, steps: 4\n",
      "Episode 63: reward: 2.600, steps: 27\n",
      "Episode 64: reward: 1.400, steps: 15\n",
      "Episode 65: reward: 1.200, steps: 13\n",
      "Episode 66: reward: 2.500, steps: 26\n",
      "Episode 67: reward: 66.400, steps: 5\n",
      "Episode 68: reward: 2.100, steps: 20\n",
      "Episode 69: reward: 0.100, steps: 2\n",
      "Episode 70: reward: 0.300, steps: 4\n",
      "Episode 71: reward: 3.100, steps: 31\n",
      "Episode 72: reward: 33.600, steps: 7\n",
      "Episode 73: reward: 69.600, steps: 36\n",
      "Episode 74: reward: 38.000, steps: 50\n",
      "Episode 75: reward: 0.500, steps: 6\n",
      "Episode 76: reward: 0.300, steps: 4\n",
      "Episode 77: reward: 2.400, steps: 23\n",
      "Episode 78: reward: 0.100, steps: 2\n",
      "Episode 79: reward: 0.700, steps: 8\n",
      "Episode 80: reward: 0.300, steps: 4\n",
      "Episode 81: reward: 0.100, steps: 2\n",
      "Episode 82: reward: 1.000, steps: 11\n",
      "Episode 83: reward: 0.500, steps: 6\n",
      "Episode 84: reward: 0.500, steps: 6\n",
      "Episode 85: reward: 1.300, steps: 13\n",
      "Episode 86: reward: 0.100, steps: 2\n",
      "Episode 87: reward: 0.800, steps: 9\n",
      "Episode 88: reward: 2.500, steps: 25\n",
      "Episode 89: reward: 0.100, steps: 2\n",
      "Episode 90: reward: 2.400, steps: 25\n",
      "Episode 91: reward: 0.800, steps: 9\n",
      "Episode 92: reward: 33.700, steps: 7\n",
      "Episode 93: reward: 0.200, steps: 3\n",
      "Episode 94: reward: 0.400, steps: 5\n",
      "Episode 95: reward: 1.100, steps: 12\n",
      "Episode 96: reward: 1.900, steps: 20\n",
      "Episode 97: reward: 2.000, steps: 21\n",
      "Episode 98: reward: 0.800, steps: 8\n",
      "Episode 99: reward: 35.800, steps: 29\n",
      "Episode 100: reward: 0.600, steps: 7\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes = 100, visualize = False)\n",
    "dqn.save_weights('Large_weights.h5',overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be17574a-90cb-43ba-925c-d1c7c2c3de06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
